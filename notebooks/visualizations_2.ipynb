{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mall_ids\u001b[49m), batch_size):\n\u001b[1;32m     34\u001b[0m     batch_ids \u001b[38;5;241m=\u001b[39m all_ids[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[1;32m     35\u001b[0m     response \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mfetch(ids\u001b[38;5;241m=\u001b[39mbatch_ids, namespace\u001b[38;5;241m=\u001b[39mnamespace)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Pinecone API key and initialize\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not pinecone_api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not set in the .env file\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index_name = 'idea-index'\n",
    "namespace = os.getenv(\"PINECONE_NAMESPACE\")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Fetch ideas and embeddings\n",
    "ideas = []  # Placeholder for actual idea text\n",
    "embeddings = []\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(all_ids), batch_size):\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    response = index.fetch(ids=batch_ids, namespace=namespace)\n",
    "    \n",
    "    # Extract only the \"values\" from each vector\n",
    "    embeddings.extend([vector[\"values\"] for vector in response[\"vectors\"].values()])\n",
    "    ideas.extend([vector[\"metadata\"][\"title\"] for vector in response[\"vectors\"].values()])  # Adjust based on your metadata schema\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "embeddings = np.array(embeddings, dtype=np.float32)  # Ensure data type consistency\n",
    "\n",
    "# Step 1: Scale the embeddings\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "# Step 2: Dimensionality reduction (Optional)\n",
    "pca = PCA(n_components=50)\n",
    "reduced_embeddings = pca.fit_transform(scaled_embeddings)\n",
    "\n",
    "# Step 3: Apply DBSCAN\n",
    "# Set hyperparameters: eps (distance threshold) and min_samples (minimum points per cluster)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine')\n",
    "labels = dbscan.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Step 4: Visualization using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(reduced_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels, cmap='tab20', s=50)\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(\"DBSCAN Clustering of Ideas\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Interpretation and Insights\n",
    "# Map ideas to clusters\n",
    "unique_labels = set(labels)\n",
    "clustered_ideas = {cluster: [] for cluster in unique_labels if cluster != -1}  # Exclude noise points (-1)\n",
    "noise_points = [idea for idea, label in zip(ideas, labels) if label == -1]\n",
    "\n",
    "for idea, cluster in zip(ideas, labels):\n",
    "    if cluster != -1:  # Exclude noise points\n",
    "        clustered_ideas[cluster].append(idea)\n",
    "\n",
    "# Generate insights\n",
    "for cluster, idea_list in clustered_ideas.items():\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    print(f\"Number of ideas: {len(idea_list)}\")\n",
    "    \n",
    "    # Show the most common words (optional)\n",
    "    all_words = \" \".join(idea_list).lower().split()\n",
    "    common_words = Counter(all_words).most_common(5)\n",
    "    print(f\"Top words: {common_words}\")\n",
    "    \n",
    "    # Sample ideas\n",
    "    print(\"Sample ideas:\")\n",
    "    for sample_idea in idea_list[:3]:  # Show first 3 ideas in this cluster\n",
    "        print(f\"- {sample_idea}\")\n",
    "    \n",
    "    # Actionable insights\n",
    "    print(f\"Insight: Cluster {cluster} appears to focus on {' and '.join([word[0] for word in common_words])}.\")\n",
    "\n",
    "# Step 6: Save results for further analysis\n",
    "results_df = pd.DataFrame({\"Idea\": ideas, \"Cluster\": labels})\n",
    "results_df.to_csv(\"dbscan_clustered_ideas.csv\", index=False)\n",
    "print(\"\\nClustered ideas saved to 'dbscan_clustered_ideas.csv'.\")\n",
    "\n",
    "# Step 7: Analyze Noise\n",
    "print(f\"\\nNoise points (ideas that don't belong to any cluster): {len(noise_points)}\")\n",
    "print(\"Sample noise points:\")\n",
    "for noise in noise_points[:5]:\n",
    "    print(f\"- {noise}\")\n",
    "\n",
    "print(\"Embedding shape:\", np.array(embeddings).shape)\n",
    "print(\"Sample embedding:\", embeddings[:3])\n",
    "\n",
    "embedding_matrix = np.array(embeddings)\n",
    "variance = np.var(embedding_matrix, axis=0)\n",
    "print(\"Variance across dimensions:\", variance)\n",
    "print(\"Mean variance:\", np.mean(variance))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
