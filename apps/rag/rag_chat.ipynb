{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a RAG search engine powered by Openai and Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Query: show me the ideas for integrating chatgpt\n",
      "\n",
      "\n",
      "Cosine Query: Searching for creative implementations, projects, or strategies for integrating ChatGPT or similar AI technologies into applications, systems, platforms, enhancing communication, automating customer support, providing virtual assistance, enriching user interaction, or driving business efficiency through AI-driven solutions.\n",
      "\n",
      "\n",
      "Score      - [Code] - Title\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Matches: 100%|██████████| 10/10 [00:00<00:00, 55553.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5426     - [D18104] - Chat Bot Chatbot - add to the platform \n",
      "0.5378     - [D25732] - Use AI Text generator for Brightidea content development\n",
      "0.4992     - [D26102] - Add the AI Writer from Memo to the Idea Submission from Description \n",
      "0.4876     - [D26508] - Smarter Platform\n",
      "0.4661     - [D26586] - Intelligent Assistant for BI Admins and Users\n",
      "0.4472     - [D26553] - Generative AI to add value  on a submitted idea\n",
      "0.4432     - [D26747] - Leverage AI for activity-level comment and idea description sentiment and analysis.\n",
      "0.4419     - [D19469] - Brightidea \"Inspiration Engine\" for Employee users\n",
      "0.4348     - [D16730] - Text Analytics\n",
      "0.434      - [D18586] - Suggested or auto tagging via image and text analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI BOT:\n",
      "# Summary\n",
      "Several ideas focus on integrating AI and chatbot capabilities, including utilizing ChatGPT and similar technologies to enhance conversation, content generation, and idea submission processes on the Brightidea platform.\n",
      "\n",
      "**All Ideas:**\n",
      "\n",
      "- **D18104:** Suggests adding chatbot capabilities to the Brightidea platform to assist with search and FAQs. This can be customized for end users by admins or with BI's technical services assistance.\n",
      "  \n",
      "- **D26586:** Proposes an Intelligent Assistant for BI Admins and Users utilizing Retrieval-Augmented Generation, offering context-aware support based on Knowledge Base Articles to improve interaction and operational efficiency.\n",
      "  \n",
      "- **D26102:** Involves using AI Writer from Memo in the Description field of the Idea Submission form. This would leverage AI tools like ChatGPT to assist users with articulating and clarifying their ideas.\n",
      "  \n",
      "- **D26553:** Covers the use of generative AI to enhance idea submissions by automatically gathering details, potentially enriching the initial idea post-submission.\n",
      "\n",
      "These ideas collectively aim to integrate advanced AI technologies like ChatGPT to streamline processes, improve user engagement, and support efficient knowledge management within the Brightidea platform.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Pinecone API key and initialize\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not pinecone_api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not set in the .env file\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Connect to the index\n",
    "index_name = 'idea-index'\n",
    "namespace = os.getenv(\"PINECONE_NAMESPACE\")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Define functions\n",
    "def print_matches(pinecone_results):\n",
    "    \"\"\"Format and print matches with Score, Code, and Title.\"\"\"\n",
    "    matches = pinecone_results.get(\"matches\", [])\n",
    "    if not matches:\n",
    "        print(\"\\nNo matches found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'Score':<10} - [Code] - Title\")\n",
    "    print(\"-\" * 50)\n",
    "    for match in tqdm(matches, desc=\"Processing Matches\"):\n",
    "        score = round(match[\"score\"], 4)  # Limit score to 4 decimal places\n",
    "        code = match.get(\"metadata\", {}).get(\"code\", \"N/A\")  # Handle missing codes\n",
    "        title = match.get(\"metadata\", {}).get(\"title\", \"No Title Available\")  # Handle missing titles\n",
    "        print(f\"{score:<10} - [{code}] - {title}\")\n",
    "\n",
    "def get_embeddings(text, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"Generate embeddings using OpenAI.\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def get_chat_completion(messages, model, response_format=None):\n",
    "    \"\"\"Generate chat completion using OpenAI.\"\"\"\n",
    "    completion_params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    if response_format:\n",
    "        completion_params[\"response_format\"] = response_format\n",
    "\n",
    "    completion = client.chat.completions.create(**completion_params)\n",
    "\n",
    "    result = completion.choices[0].message.content\n",
    "\n",
    "    return result\n",
    "\n",
    "def build_cosine_query(query_text):\n",
    "    \"\"\"Build a cosine similarity query.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": \"You are a helpful innovation assistant. Format my question in a way that would get the best results from a vector search of ideas in my database. The ideas have been embedded with their Title, Description, and Comments combined and lemmatized before uploading. I am going to take the output of this and give it to Pinecone to perform a cosine similarity search.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I want to find ideas related to renewable energy sources.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Looking for innovative ideas, projects, or discussions related to renewable energy sources (e.g. solar, wind, hydroelectric, geothermal, biomass), clean energy technologies, sustainable power generation, reduced carbon footprint, zero-emission strategies, and other eco-friendly solutions.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are some innovative ideas for increasing revenue?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Seeking creative and novel concepts, strategies, or initiatives to boost income, generate profits, enhance financial performance, drive sales growth, improve monetization, optimize revenue streams, or maximize earnings.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How can I improve customer satisfaction in my business?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Exploring methods, techniques, approaches, or practices to enhance customer experience, increase client happiness, elevate service quality, improve customer relations, foster client loyalty, or deliver exceptional customer service.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are some ideas for reducing waste in manufacturing processes?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Investigating innovative solutions, strategies, technologies, or methodologies to minimize waste production, reduce resource consumption, optimize material efficiency, enhance sustainability, or implement eco-friendly practices in manufacturing operations.\"},\n",
    "        {\"role\": \"user\", \"content\": query_text}\n",
    "    ]\n",
    "\n",
    "    response_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"query_schema\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"result\": {\n",
    "                        \"description\": \"The reformatted query string to use for cosine similarity search on the vector database.\",\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = get_chat_completion(messages, model=\"gpt-4o\", response_format=response_format)\n",
    "\n",
    "    # Parse the JSON response\n",
    "    response_json = json.loads(response)\n",
    "\n",
    "    query = response_json[\"result\"]  # Safely access \"result\" key from JSON response\n",
    "\n",
    "    return query\n",
    "\n",
    "def query_pinecone(query_text, index, top_k=10):\n",
    "    \"\"\"Query Pinecone index with a text input.\"\"\"\n",
    "    query_embedding = get_embeddings(query_text)\n",
    "    response = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    # Convert response to a dictionary\n",
    "    response_dict = response.to_dict()  # Use Pinecone's `to_dict()` method if available\n",
    "\n",
    "    return response_dict\n",
    "\n",
    "def generate_response(query_text, response_dict):\n",
    "    \"\"\"Generate a response based on the retrieved ideas.\"\"\"\n",
    "\n",
    "    initiative_title = \"Brightidea Product Requests\"\n",
    "    initiative_desc = \"One location for all internal enhancement and new feature requests for our product.\"\n",
    "    company = \"Brightidea is a leading idea and innovation management software designed to help organizations collect, evaluate, and implement employee ideas, fostering a culture of innovation and collaboration.\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": (\n",
    "                f\"You are a helpful innovation assistant tasked with analyzing and summarizing ideas for the intiative {initiative_title}, {initiative_desc}. {company} \\n\"\n",
    "                f\"{json.dumps(response_dict, indent=2)}\\n\"  # Serialize response to JSON format\n",
    "                \"### Instructions:\\n\"\n",
    "                \"- Focus your response on addressing the user's query clearly and comprehensively.\\n\"\n",
    "                \"- Create a concise narrative summarizing the most relevant ideas, ensuring the response is easy to read and avoids simply listing items.\\n\"\n",
    "                \"- If no ideas are directly relevant, offer general guidance or propose methods to refine the query.\\n\\n\"\n",
    "                \"### Example Format:\\n\"\n",
    "                \"# Summary\\n\"\n",
    "                \"Brief overview of the key themes and notable ideas. Highlight the most promising suggestions.\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query_text\n",
    "        }\n",
    "\n",
    "    ]\n",
    "\n",
    "    respone_format = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"response_schema\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"response\": {\n",
    "                        \"description\": \"The response to the user's query based on the retrieved ideas.\",\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = get_chat_completion(messages, model=\"gpt-4o\", response_format=respone_format)\n",
    "\n",
    "    # Parse the JSON response\n",
    "    response_json = json.loads(response)\n",
    "\n",
    "    query = response_json[\"response\"]  # Safely access \"result\" key from JSON response\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Interactive Cells\n",
    "# Query Example\n",
    "query_text = input(\"\\nEnter a query to search the ideaspace: \")\n",
    "\n",
    "print(f\"\\nUser Query: {query_text}\\n\")\n",
    "\n",
    "cosine_query = build_cosine_query(query_text)\n",
    "print(f\"\\nCosine Query: {cosine_query}\\n\")\n",
    "\n",
    "pinecone_results = query_pinecone(cosine_query, index)\n",
    "print_matches(pinecone_results)\n",
    "\n",
    "response = generate_response(query_text, pinecone_results)\n",
    "\n",
    "print(f\"BI BOT:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
