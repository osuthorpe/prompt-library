{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "from umap.umap_ import UMAP\n",
    "from dotenv import load_dotenv\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# Step 1: Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Pinecone API key and initialize\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not pinecone_api_key:\n",
    "    raise ValueError(\"PINECONE_API_KEY is not set in the .env file\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "index_name = 'idea-index'\n",
    "namespace = os.getenv(\"PINECONE_NAMESPACE\")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Step 2: Fetch embeddings from Pinecone\n",
    "batch_size = 500\n",
    "all_ids = []\n",
    "for ids in index.list(namespace=namespace):\n",
    "    all_ids.extend(ids)\n",
    "\n",
    "embeddings = []\n",
    "for i in range(0, len(all_ids), batch_size):\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    response = index.fetch(ids=batch_ids, namespace=namespace)\n",
    "    embeddings.extend([vector[\"values\"] for vector in response[\"vectors\"].values()])\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "# Step 3: Fetch metadata from Pinecone\n",
    "metadata_dict = {}\n",
    "for i in range(0, len(all_ids), batch_size):\n",
    "    batch_ids = all_ids[i:i + batch_size]\n",
    "    response = index.fetch(ids=batch_ids, namespace=namespace)\n",
    "    for vector_id, vector_data in response['vectors'].items():\n",
    "        metadata_dict[vector_id] = vector_data['metadata']\n",
    "\n",
    "# Convert metadata to a DataFrame\n",
    "metadata_df = pd.DataFrame.from_dict(metadata_dict, orient='index')\n",
    "metadata_df.reset_index(inplace=True)\n",
    "metadata_df.rename(columns={'index': 'vector_id'}, inplace=True)\n",
    "\n",
    "# Step 4: Combine text fields for NLP processing\n",
    "metadata_df['combined_text'] = metadata_df[['description', 'title', 'comments']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', TfidfVectorizer(max_features=500), 'combined_text'),\n",
    "        ('category', OneHotEncoder(handle_unknown='ignore'), ['category'])\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    verbose_feature_names_out=False  # Optional if verbose warnings appear\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "transformed_metadata = pipeline.fit_transform(metadata_df)\n",
    "\n",
    "# Combine embeddings with transformed metadata\n",
    "scaled_embeddings = StandardScaler().fit_transform(embeddings)\n",
    "combined_data = np.hstack([scaled_embeddings, transformed_metadata.toarray()])\n",
    "\n",
    "# Step 5: Dimensionality reduction with PCA\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "reduced_data = pca.fit_transform(combined_data)\n",
    "\n",
    "# Check explained variance\n",
    "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Cumulative variance explained by 50 PCA components: {explained_variance:.2f}\")\n",
    "\n",
    "# Step 6: Determine optimal clusters using Elbow Method\n",
    "sse = []\n",
    "k_values = range(2, 30)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(reduced_data)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    labels = kmeans.fit_predict(reduced_data)\n",
    "    silhouette_avg = silhouette_score(reduced_data, labels)\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.title(\"Elbow Method: Optimal Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Clustering with K-Means\n",
    "optimal_clusters = 12  # Adjust based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "# Step 8: Evaluate clustering\n",
    "davies_bouldin = davies_bouldin_score(reduced_data, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(reduced_data, labels)\n",
    "silhouette = silhouette_score(reduced_data, labels)\n",
    "\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.2f}\")  # Lower is better\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.2f}\")  # Higher is better\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")  # Higher is better\n",
    "\n",
    "# Step 9: Visualization with UMAP\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "umap_results = umap.fit_transform(reduced_data)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(umap_results[:, 0], umap_results[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(\"Clustering of Ideas (UMAP Visualization)\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotate clusters\n",
    "for i in range(optimal_clusters):\n",
    "    cluster_indices = np.where(labels == i)[0]\n",
    "    plt.text(umap_results[cluster_indices[0], 0], umap_results[cluster_indices[0], 1],\n",
    "             f\"Cluster {i}\", color='black', fontsize=12, ha='center', va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Save results\n",
    "metadata_df['Cluster'] = labels\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"clustering_results.csv\")\n",
    "metadata_df.to_csv(desktop_path, index=False)\n",
    "print(f\"Clustering results saved to {desktop_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering Results:\n",
    "\n",
    "Cluster 0: Issues related to visibility and syncing across components (e.g., items not appearing in search, challenges of maintaining consistency across submissions).\n",
    "Cluster 1: Feedback and user engagement features (e.g., feedback on voting behaviors, Slack integration for evaluations).\n",
    "Cluster 2: Weighted scoring and pipeline notifications.\n",
    "Cluster 3: Administrative restrictions and assignment configurations.\n",
    "Cluster 4: Customization of views and resolving CSS/UI issues.\n",
    "Cluster 5: Tagging and metadata automation.\n",
    "Cluster 6: Language selection, admin onboarding, and platform analytics.\n",
    "Cluster 7: Notification and action handling across initiatives.\n",
    "Cluster 8: Access management and permissions for communities and challenges.\n",
    "Cluster 9: Workflow enhancements and roadmap development tools.\n",
    "Cluster 10: End-user surveys and better participant reporting.\n",
    "Cluster 11: Simplification of setup and interface design improvements.\n",
    "Cluster 12: Editing permissions and voting enhancements.\n",
    "Cluster 13: General UI/UX design issues and cleanup.\n",
    "Cluster 14: Consistency in functionality and idea creation tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
